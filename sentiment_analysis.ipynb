{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import word2vec\n",
    "import re\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-cb818ecd6623>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-cb818ecd6623>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    word2vec.word2vec(vecfile, '/home/claire/Documents/jobs/job-hunting/massively/text8.bin', size=100, verbose=True)model = word2vec.load('/home/claire/Documents/jobs/job-hunting/massively/text8.bin')\u001b[0m\n\u001b[0m                                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# train word2vec with sample word list dataset\n",
    "vecfile = '/home/claire/Documents/jobs/job-hunting/massively/text8'\n",
    "word2vec.word2vec(vecfile, '/home/claire/Documents/jobs/job-hunting/massively/text8.bin', size=100, verbose=True)\n",
    "model = word2vec.load('/home/claire/Documents/jobs/job-hunting/massively/text8.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = word2vec.load('/home/claire/Documents/jobs/job-hunting/massively/text8.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the sentiment data\n",
    "df = pd.DataFrame.from_csv('/home/claire/Documents/jobs/job-hunting/massively/dfe_happysad_utf.csv', header=0, sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into test and train\n",
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "\n",
    "msk = np.random.rand(len(df)) <= 0.7\n",
    "\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train = df_train.as_matrix(columns=df.columns[0:2])\n",
    "np_test = df_test.as_matrix(columns=df.columns[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum sentence length and number of dimension for each word vector\n",
    "maxSeqLength = df_train['features'].str.len().max()\n",
    "numDimensions = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an IDs matrix that is nOfReviews * maxSeqLength\n",
    "ids = np.zeros((nOfReviews, maxSeqLength), dtype='int32')\n",
    "# reviews = a set of all the reviews \n",
    "\n",
    "# do this for each review at each word, for positive reviews and negative reviews\n",
    "for word in tweet:\n",
    "    try:\n",
    "        ids[tweet][index] = wordsList.index(word)\n",
    "    except ValueError:\n",
    "        #set vector for unkown words\n",
    "        ids[tweet][index] = 399999 \n",
    "\n",
    "# return IDs matrix here\n",
    "# save with np.save(name, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set params for logistic regression model\n",
    "\n",
    "base_columns = [\n",
    "    'feature', 'label'\n",
    "]\n",
    "m.train(\n",
    "    input_fn=input_fn(train_file_name, num_epochs=None, shuffle=True),\n",
    "    steps=train_steps)\n",
    "model_dir = tempfile.mkdtemp()\n",
    "m = tf.estimator.LinearClassifier(\n",
    "    model_dir=model_dir, feature_columns=base_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "m.train(\n",
    "    input_fn=input_fn(train_file_name, num_epochs=None, shuffle=True),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "# this returns the model's accuracy so it can be improved upon\n",
    "results = m.evaluate(\n",
    "    input_fn=input_fn(test_file_name, num_epochs=1, shuffle=False),\n",
    "    steps=None)\n",
    "print(\"model directory = %s\" % model_dir)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
